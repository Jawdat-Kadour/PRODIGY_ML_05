{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.utils import class_weight\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import backend as K\n","import os\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.applications import EfficientNetB3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_classes = 101\n","img_width, img_height = 224, 224\n","batch_size = 16\n","\n","train_data_dir = 'data-101/train'\n","validation_data_dir = 'data-101/val'\n","test_data_dir = 'data-101/test'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Count samples\n","def count_samples(directory):\n","    total = 0\n","    for class_name in os.listdir(directory):\n","        class_dir = os.path.join(directory, class_name)\n","        total += len(os.listdir(class_dir))\n","    return total\n","\n","nb_train_samples = count_samples(train_data_dir)\n","nb_validation_samples = count_samples(validation_data_dir)\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    shear_range=0.2,  \n","    zoom_range=0.2,   \n","    rotation_range=20, \n","    width_shift_range=0.2, \n","    height_shift_range=0.2, \n","    brightness_range=[0.9, 1.1],  \n","    horizontal_flip=True)\n","\n","\n","\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n"]},{"cell_type":"markdown","metadata":{"_uuid":"3a9e9afbfe1fc303c23c06c27444b66988ab8e9d","id":"upx61ukJiA8B"},"source":["### **Fine tune ResNet50 Pretrained model using Food 101 dataset**"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 480 images belonging to 3 classes.\n","Found 60 images belonging to 3 classes.\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\mudar\\anaconda3\\envs\\Tenserflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","30/30 [==============================] - ETA: 0s - loss: 1.6322 - accuracy: 0.3625\n","Epoch 1: val_loss improved from inf to 1.13693, saving model to working\\best_model_3class.hdf5\n","30/30 [==============================] - 56s 1s/step - loss: 1.6322 - accuracy: 0.3625 - val_loss: 1.1369 - val_accuracy: 0.2708 - lr: 1.0000e-04\n","Epoch 2/20\n","30/30 [==============================] - ETA: 0s - loss: 1.5034 - accuracy: 0.3729\n","Epoch 2: val_loss improved from 1.13693 to 1.11953, saving model to working\\best_model_3class.hdf5\n","30/30 [==============================] - 38s 1s/step - loss: 1.5034 - accuracy: 0.3729 - val_loss: 1.1195 - val_accuracy: 0.3542 - lr: 1.0000e-04\n","Epoch 3/20\n","30/30 [==============================] - ETA: 0s - loss: 1.4680 - accuracy: 0.3896\n","Epoch 3: val_loss did not improve from 1.11953\n","30/30 [==============================] - 33s 1s/step - loss: 1.4680 - accuracy: 0.3896 - val_loss: 1.1325 - val_accuracy: 0.3958 - lr: 1.0000e-04\n","Epoch 4/20\n","30/30 [==============================] - ETA: 0s - loss: 1.4598 - accuracy: 0.3708\n","Epoch 4: val_loss did not improve from 1.11953\n","30/30 [==============================] - 36s 1s/step - loss: 1.4598 - accuracy: 0.3708 - val_loss: 1.1395 - val_accuracy: 0.3958 - lr: 1.0000e-04\n","Epoch 5/20\n","30/30 [==============================] - ETA: 0s - loss: 1.4235 - accuracy: 0.3750\n","Epoch 5: val_loss did not improve from 1.11953\n","30/30 [==============================] - 37s 1s/step - loss: 1.4235 - accuracy: 0.3750 - val_loss: 1.2543 - val_accuracy: 0.3542 - lr: 1.0000e-04\n","Epoch 6/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3668 - accuracy: 0.3875\n","Epoch 6: val_loss did not improve from 1.11953\n","30/30 [==============================] - 32s 1s/step - loss: 1.3668 - accuracy: 0.3875 - val_loss: 1.1820 - val_accuracy: 0.3542 - lr: 1.0000e-04\n","Epoch 7/20\n","30/30 [==============================] - ETA: 0s - loss: 1.4323 - accuracy: 0.3896\n","Epoch 7: val_loss did not improve from 1.11953\n","\n","Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n","30/30 [==============================] - 35s 1s/step - loss: 1.4323 - accuracy: 0.3896 - val_loss: 1.2259 - val_accuracy: 0.2917 - lr: 1.0000e-04\n","Epoch 8/20\n","30/30 [==============================] - ETA: 0s - loss: 1.4311 - accuracy: 0.3896\n","Epoch 8: val_loss did not improve from 1.11953\n","30/30 [==============================] - 42s 1s/step - loss: 1.4311 - accuracy: 0.3896 - val_loss: 1.2692 - val_accuracy: 0.2917 - lr: 5.0000e-05\n","Epoch 9/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3499 - accuracy: 0.3667\n","Epoch 9: val_loss did not improve from 1.11953\n","30/30 [==============================] - 35s 1s/step - loss: 1.3499 - accuracy: 0.3667 - val_loss: 1.2400 - val_accuracy: 0.3542 - lr: 5.0000e-05\n","Epoch 10/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3361 - accuracy: 0.3854\n","Epoch 10: val_loss did not improve from 1.11953\n","30/30 [==============================] - 47s 2s/step - loss: 1.3361 - accuracy: 0.3854 - val_loss: 1.1670 - val_accuracy: 0.3750 - lr: 5.0000e-05\n","Epoch 11/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3327 - accuracy: 0.3854\n","Epoch 11: val_loss improved from 1.11953 to 1.10354, saving model to working\\best_model_3class.hdf5\n","30/30 [==============================] - 42s 1s/step - loss: 1.3327 - accuracy: 0.3854 - val_loss: 1.1035 - val_accuracy: 0.3125 - lr: 5.0000e-05\n","Epoch 12/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3971 - accuracy: 0.3625\n","Epoch 12: val_loss improved from 1.10354 to 1.08468, saving model to working\\best_model_3class.hdf5\n","30/30 [==============================] - 50s 2s/step - loss: 1.3971 - accuracy: 0.3625 - val_loss: 1.0847 - val_accuracy: 0.3958 - lr: 5.0000e-05\n","Epoch 13/20\n","30/30 [==============================] - ETA: 0s - loss: 1.2676 - accuracy: 0.4021\n","Epoch 13: val_loss improved from 1.08468 to 1.05830, saving model to working\\best_model_3class.hdf5\n","30/30 [==============================] - 55s 2s/step - loss: 1.2676 - accuracy: 0.4021 - val_loss: 1.0583 - val_accuracy: 0.5417 - lr: 5.0000e-05\n","Epoch 14/20\n","30/30 [==============================] - ETA: 0s - loss: 1.2823 - accuracy: 0.4187\n","Epoch 14: val_loss did not improve from 1.05830\n","30/30 [==============================] - 49s 2s/step - loss: 1.2823 - accuracy: 0.4187 - val_loss: 1.0906 - val_accuracy: 0.4792 - lr: 5.0000e-05\n","Epoch 15/20\n","30/30 [==============================] - ETA: 0s - loss: 1.2677 - accuracy: 0.4083\n","Epoch 15: val_loss improved from 1.05830 to 1.05066, saving model to working\\best_model_3class.hdf5\n","30/30 [==============================] - 55s 2s/step - loss: 1.2677 - accuracy: 0.4083 - val_loss: 1.0507 - val_accuracy: 0.5625 - lr: 5.0000e-05\n","Epoch 16/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3116 - accuracy: 0.3708\n","Epoch 16: val_loss did not improve from 1.05066\n","30/30 [==============================] - 48s 2s/step - loss: 1.3116 - accuracy: 0.3708 - val_loss: 1.0601 - val_accuracy: 0.5208 - lr: 5.0000e-05\n","Epoch 17/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3356 - accuracy: 0.3667\n","Epoch 17: val_loss did not improve from 1.05066\n","30/30 [==============================] - 48s 2s/step - loss: 1.3356 - accuracy: 0.3667 - val_loss: 1.0539 - val_accuracy: 0.5000 - lr: 5.0000e-05\n","Epoch 18/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3003 - accuracy: 0.3854\n","Epoch 18: val_loss improved from 1.05066 to 1.01144, saving model to working\\best_model_3class.hdf5\n","30/30 [==============================] - 54s 2s/step - loss: 1.3003 - accuracy: 0.3854 - val_loss: 1.0114 - val_accuracy: 0.5833 - lr: 5.0000e-05\n","Epoch 19/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3386 - accuracy: 0.3333\n","Epoch 19: val_loss did not improve from 1.01144\n","30/30 [==============================] - 40s 1s/step - loss: 1.3386 - accuracy: 0.3333 - val_loss: 1.0398 - val_accuracy: 0.4792 - lr: 5.0000e-05\n","Epoch 20/20\n","30/30 [==============================] - ETA: 0s - loss: 1.3581 - accuracy: 0.3604\n","Epoch 20: val_loss did not improve from 1.01144\n","30/30 [==============================] - 36s 1s/step - loss: 1.3581 - accuracy: 0.3604 - val_loss: 1.0690 - val_accuracy: 0.4792 - lr: 5.0000e-05\n"]}],"source":["# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n","\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation='relu')(x)\n","x = BatchNormalization()(x)  # Batch normalization\n","x = Dropout(0.5)(x)\n","x = Dense(512, activation='relu')(x)\n","x = BatchNormalization()(x)  # Batch normalization\n","predictions = Dense(n_classes, activation='softmax')(x) \n","\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","for layer in base_model.layers[:-50]:  # Unfreeze more layers\n","    layer.trainable = False\n","\n","for layer in base_model.layers[-50:]:\n","    layer.trainable = True\n","\n","\n","model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","checkpointer = ModelCheckpoint(filepath='working/best_model_3class.hdf5', verbose=1, save_best_only=True)\n","csv_logger = CSVLogger('working/history_3class.log')\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n","\n","\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=nb_train_samples // batch_size,\n","    validation_data=validation_generator,\n","    validation_steps=nb_validation_samples // batch_size,\n","    epochs=20,\n","    callbacks=[csv_logger, checkpointer, early_stopping, reduce_lr])\n","model.save('working/model_trained_3class.hdf5')"]},{"cell_type":"markdown","metadata":{},"source":["Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_generator = test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(img_width, img_height),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    shuffle=False)  \n","\n","test_loss, test_accuracy = model.evaluate(test_generator)\n","print(f\"Test accuracy: {test_accuracy}, Test loss: {test_loss}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","# Predict on test data\n","test_generator.reset()\n","predictions = model.predict(test_generator)\n","predicted_classes = np.argmax(predictions, axis=1)\n","true_classes = test_generator.classes\n","class_labels = list(test_generator.class_indices.keys())\n","\n","conf_matrix = confusion_matrix(true_classes, predicted_classes)\n","print(conf_matrix)\n","\n","report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n","print(report)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.preprocessing import image\n","import numpy as np\n","\n","def load_and_prep_image(img_path, img_size):\n","    img = image.load_img(img_path, target_size=img_size)\n","    img_array = image.img_to_array(img)\n","    img_array = np.expand_dims(img_array, axis=0)  # Reshape for model input\n","    img_array /= 255.0  \n","    return img_array\n","\n","img_path = '.jpg'\n","img = load_and_prep_image(img_path, (img_width, img_height))\n","prediction = model.predict(img)\n","predicted_class = class_labels[np.argmax(prediction)]\n","print(f\"Predicted class: {predicted_class}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T07:28:48.998079Z","iopub.status.busy":"2023-06-04T07:28:48.99761Z","iopub.status.idle":"2023-06-04T07:28:54.904513Z","shell.execute_reply":"2023-06-04T07:28:54.903508Z","shell.execute_reply.started":"2023-06-04T07:28:48.998028Z"},"trusted":true},"outputs":[],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-04T07:28:54.90667Z","iopub.status.busy":"2023-06-04T07:28:54.906342Z","iopub.status.idle":"2023-06-04T07:28:54.914018Z","shell.execute_reply":"2023-06-04T07:28:54.913126Z","shell.execute_reply.started":"2023-06-04T07:28:54.906615Z"},"trusted":true},"outputs":[],"source":["import openai\n","\n","\n","# openai.api_key =\n","\n","def get_calories(food_item):\n","    prompt = f\"Write the calories in {food_item} per 100g. The answer must be in the following format: <number> calories. Provide no explanations or anything. You must give the answer in the aforementioned format.\"\n","    model_engine = \"text-davinci-002\"\n","    completions = openai.Completion.create(engine=model_engine, prompt=prompt, max_tokens=1024, n=1,stop=None,temperature=0.5)\n","    message = completions.choices[0].text\n","    return message.strip()\n","\n","food_item = \"banana\"\n","calories = get_calories(food_item)\n","print(f\"The number of calories in an {food_item} is {calories}\")"]},{"cell_type":"markdown","metadata":{"_uuid":"2d1e408e84fd5260bb323fb6d3678a3a0df3f271"},"source":["* **Model training on all 101 classes takes some time**\n","* **It was taking more than an hour for one epoch when the full dataset is used for fine tuning**"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":8544,"sourceId":11959,"sourceType":"datasetVersion"}],"dockerImageVersionId":21695,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
